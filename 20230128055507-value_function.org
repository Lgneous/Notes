:PROPERTIES:
:ID:       f3d77d30-d6d0-47e1-99a1-3895f726efac
:END:
#+title: Value Function
#+filetags: :machine-learning:markov:reinforcement-learning:

Related: [[id:c857ab3f-2207-43fa-9f01-8ac2579c5b85][Policies]]

The value function of a state $s$ under a policy $\pi$, denoted $v_{\pi}(s)$, is the expected return when starting in state $s$ and following $\pi$ thereafter.
\begin{aligned}
v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]=\mathbb{E}_\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t=s]
\end{aligned}
Not that the value of the terminal state is always 0, we call $v_\pi$ the state-value function under policy $\pi$.

Similarly, we define the value of taking an action $a$ in state $s$ under policy $\pi$, denoted $q_\pi(s, a)$, as the expected return starting in state $s$, taking the action $a$ and following $\pi$ thereafter.
\begin{aligned}
q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]=\mathbb{E}_\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t=s,A_t=a]
\end{aligned}
[[id:e37948bb-0480-41f4-9c9a-5bfe2536c1d7][Q-Learning]] is a method for estimating $q_\pi(s,a)$.

If the current state is $S_t$, and actions are selected according to a stochastic policy $\pi$, then what is the expectation of $R_{t+1}$ in terms of $\pi$ and the four-argument function $p$?
\begin{aligned}
\mathbb{E}[R_{t+1}|S_{t}=s]=\sum_{a}\pi(a|S_{t})\sum_{s',r}r*p(s',r|s,a)$
\end{aligned}
Intuitively, $v_\pi(s)$, the expected return given a state $s$ is the reward $r$ given by the transition to state $s'$ with action $a$ multiplied by the probability that the action $a$ gives rise to state $s'$, weighted by the policy $\pi$.

$q_\pi(s,a)$ is the reward $r$ + the discounted future reward $\gamma v_\pi(s')$ for each possible future state-reward pairs $s',r$, that is, $v_\pi(s)$ without the need to pick the action $a$ from the policy $\pi(a|s)$.

Alternative forms of $v_\pi(s)$ (using the [[id:a92cb8d5-068e-492a-8f75-f8449f3459ec][Bellman Equation]]) and $\q_\pi(s,a)$:
\begin{aligned}
v_\pi(s) & = \mathbb{E}_\pi[G_t|S_t=s] \\
& = \mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
& = \sum_a \pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)[r+\gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']] \\
& = \sum_a \pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')] \\
& = \sum_a \pi(a|s)q_\pi(s,a) \\
q_\pi(s,a) & =   \frac{v_\pi(s)}{\sum\limits_a \pi(a|s)} \\
& =\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
\end{aligned}
