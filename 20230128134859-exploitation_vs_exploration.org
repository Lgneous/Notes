:PROPERTIES:
:ID:       c8ff1865-c7fb-4f86-9372-b4bec2e67016
:END:
#+title: Exploitation vs Exploration
#+filetags: :probability:reinforcement-learning:machine-learning:

* Definitions
- Exploration: improve knowledge for long-term benefit
- Exploitation: exploit knowledge for short-term benefit

* How to pick one or the other?
One way is to simply randomly pick with probability $\epsilon$ whether or not we pick an exploratory action.

This method is called [[id:92aa1272-b32f-4949-a222-3bed968d7c67][Epsilon-Greedy]].

Another way is to use [[id:ed5a2094-0450-4f34-984a-d17d245788a9][Upper-Confidence Bound Action Selection]].

A method using [[id:bec9a94f-1214-46f1-9a99-c5a1265f94fd][Gradient Descent]] exists in the form of [[id:19a533c3-3092-4f2b-b31f-c24f3a1fe339][Bandit Gradient Descent]]

* Initialization of the estimates
We are also concerned with finding a proper way to initialize our initial estimates.
[[id:7011e9a8-9442-478e-a474-45940acbb71a][Optimistic Initial Values]]
